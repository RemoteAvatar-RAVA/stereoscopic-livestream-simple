<!doctype html>
<!--
Copyright 2018 The Immersive Web Community Group
...
-->
<html>
  <head>
    <meta charset='utf-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1, user-scalable=no'>
    <meta name='mobile-web-app-capable' content='yes'>
    <meta name='apple-mobile-web-app-capable' content='yes'>
    <link rel='icon' type='image/png' sizes='32x32' href='favicon-32x32.png'>
    <link rel='icon' type='image/png' sizes='96x96' href='favicon-96x96.png'>
    <link rel='stylesheet' href='css/common.css'>

    <!-- MediaMTX WebRTC helper -->
    <script defer src="./reader.js"></script>

    <title>Stereo Video Player</title>
  </head>

  <body>
    <header>
      <details open>
        <summary>Stereo Video Player</summary>
        <p>
          This sample demonstrates how to play a stereo 3D video.
          <a class="back" href="./">Back</a>
          <br/>
          <hr/>
          Autoplay when VR starts: <input id='autoplayVideo' type='checkbox'/>
        </p>
      </details>
    </header>

    <script type="module">
      import {WebXRButton} from './js/util/webxr-button.js';
      import {Scene} from './js/render/scenes/scene.js';
      import {Renderer, createWebGLContext} from './js/render/core/renderer.js';
      import {UrlTexture} from './js/render/core/texture.js';
      import {ButtonNode} from './js/render/nodes/button.js';
      import {Gltf2Node} from './js/render/nodes/gltf2.js';
      import {VideoNode} from './js/render/nodes/video.js';
      import {InlineViewerHelper} from './js/util/inline-viewer-helper.js';
      import {QueryArgs} from './js/util/query-args.js';

      import WebXRPolyfill from './js/third-party/webxr-polyfill/build/webxr-polyfill.module.js';
      if (QueryArgs.getBool('usePolyfill', true)) {
        let polyfill = new WebXRPolyfill();
      }

      let autoplayCheckbox = document.getElementById('autoplayVideo');

      // XR globals.
      let xrButton = null;
      let xrImmersiveRefSpace = null;
      let inlineViewerHelper = null;

      // WebGL scene globals.
      let gl = null;
      let renderer = null;
      let scene = new Scene();
      scene.addNode(new Gltf2Node({url: 'media/gltf/home-theater/home-theater.gltf'}));
      scene.enableStats(false);

      // -----------------------------------------------------------------
      // WEBRTC SECTION
      // -----------------------------------------------------------------

      let video = document.createElement('video');
      video.dataset.src = "webrtc://live/mystream";

      // NOTE: crossOrigin is irrelevant for srcObject, but harmless
      video.crossOrigin = 'anonymous';

      // Autoplay friendly settings
      video.muted = true;
      video.autoplay = true;
      video.playsInline = true;
      video.preload = "auto";
      video.disablePictureInPicture = true;
      video.setAttribute("playsinline", "");
      video.setAttribute("muted", "");
      video.setAttribute("autoplay", "");

      // Create VideoNode early, BUT don't add to scene until video has real frames.
      let videoNode = new VideoNode({
        video: video,
        displayMode: 'stereoLeftRight'
      });

      console.log("VideoTexture debug:",
        videoNode._video_texture?.key,
        videoNode._video_texture?._key,
        video.dataset.src,
        video.currentSrc,
        video.src
      );


      // Position/scale in theatre
      videoNode.translation = [0.025, 0.275, -1.5];
      videoNode.scale = [2.1, 1.1, 1.0];

      let videoNodeAdded = false;

      function addVideoNodeOnceReady() {
        if (videoNodeAdded) return;
        videoNodeAdded = true;

        // Add to scene only after video is ready, to avoid “bind black texture once” bugs
        scene.addNode(videoNode);

        // Now that node exists in scene, attach interactions safely
        videoNode.onSelect(() => {
          if (!video.paused) {
            playButton.visible = true;
            video.pause();
          } else {
            playButton.visible = false;
            video.play().catch(()=>{});
          }
        });
        videoNode.selectable = true;

        console.log("VideoNode added to scene");
      }

      // IMPORTANT: your HLS path was /live/mystream, so WHEP should match
      const whepUrl = "http://192.168.32.72:8889/live/mystream/whep";
      let reader = null;

      function startWHEP() {
        if (typeof MediaMTXWebRTCReader === "undefined") {
          console.log("Waiting for reader.js to load...");
          setTimeout(startWHEP, 50);
          return;
        }

        console.log("Starting WHEP:", whepUrl);

        reader = new MediaMTXWebRTCReader({
          url: whepUrl,
          user: "",
          pass: "",
          token: "",

          onError: (err) => {
            console.error("WHEP/WebRTC error:", err);
          },

          // onTrack: (evt) => {
          //   console.log("onTrack:", evt);

          //   if (evt.streams && evt.streams[0]) {
          //     video.srcObject = evt.streams[0];

          //     // Try to keep playback "live"
          //     video.playbackRate = 1.0;
          //     video.defaultPlaybackRate = 1.0;

          //     // Play immediately for DOM preview
          //     video.play().then(() => {
          //       console.log("video.play() ok");
          //     }).catch((e) => {
          //       console.warn("video.play() blocked:", e);
          //     });
          //   } else {
          //     console.warn("No streams[0] in onTrack event:", evt);
          //   }
          // },

          onTrack: (evt) => {
            console.log("onTrack:", evt);

            if (evt.streams && evt.streams[0]) {
              video.srcObject = evt.streams[0];

              // Try to keep playback "live"
              video.playbackRate = 1.0;
              video.defaultPlaybackRate = 1.0;

              // >>> FORCE LIVE EDGE BEHAVIOUR (added) <<<
              // Keep currentTime close to the end of the buffered range.
              // This prevents the browser from accumulating 2–3s of latency over time.
              const LIVE_EDGE_MAX_LAG_S = 0.15; // allowed lag behind live edge
              const LIVE_EDGE_TARGET_S  = 0.05; // how close to jump to edge

              const liveEdgeClamp = () => {
                try {
                  if (!video.buffered || video.buffered.length === 0) return;

                  // Use the most recent buffered range
                  const end = video.buffered.end(video.buffered.length - 1);
                  const lag = end - video.currentTime;

                  if (lag > LIVE_EDGE_MAX_LAG_S) {
                    video.currentTime = Math.max(0, end - LIVE_EDGE_TARGET_S);
                  }
                } catch (e) {
                  // buffered.end can throw while ranges are changing
                }
              };

              // Attach once
              if (!video._liveEdgeClampAttached) {
                video._liveEdgeClampAttached = true;
                video.addEventListener("timeupdate", liveEdgeClamp);
                video.addEventListener("progress", liveEdgeClamp);
                video.addEventListener("playing", liveEdgeClamp);
              }
              // <<< end added >>>

              // Play immediately
              video.play().then(() => {
                console.log("video.play() ok");
              }).catch((e) => {
                console.warn("video.play() blocked:", e);
              });
            } else {
              console.warn("No streams[0] in onTrack event:", evt);
            }
          },
        });
      }

      startWHEP();

      // When metadata arrives, we know the stream has real dimensions.
      // This is the moment we add the node and “rebind” it once to force texture update.
      video.addEventListener("loadedmetadata", () => {
        console.log("loadedmetadata", video.videoWidth, video.videoHeight);

        // Add to scene only once real frames exist
        addVideoNodeOnceReady();

        console.log("After adding video node")

        // Adjust aspect ratio once we know real dims
        let aspect = videoNode.aspectRatio;
        if (aspect < 2.0) {
          videoNode.scale = [aspect * 1.1, 1.1, 1.0];
        } else {
          videoNode.scale = [2.1, 2.1 / aspect, 1.0];
        }

        // FORCE a rebind: remove and re-add the node.
        // This fixes many “black texture forever” cases with HTMLVideoElement -> WebGL texture.
        try {
          scene.removeNode(videoNode);
          scene.addNode(videoNode);
          console.log("VideoNode re-added to force texture rebind");
        } catch (e) {
          // removeNode may not exist in your Scene implementation; harmless
          console.warn("Scene remove/re-add not supported:", e);
        }
      }, { once: true });

      video.addEventListener("error", () => console.error("VIDEO error", video.error));
      video.addEventListener("playing", () => console.log("playing"));
      video.addEventListener("waiting", () => console.log("waiting (buffering)"));

      window.addEventListener("beforeunload", () => {
        if (reader) {
          try { reader.close(); } catch (e) {}
        }
      });

      // -----------------------------------------------------------------
      // UI interactions for the video node
      // -----------------------------------------------------------------

      let playTexture = new UrlTexture('media/textures/play-button.png');

      let playButton = new ButtonNode(playTexture, () => {
        if (video.paused) {
          playButton.visible = false;
          video.play().catch(()=>{});
        }
      });

      // Put the play button near the screen while debugging
      playButton.translation = [0.025, 0.275, -1.4];
      playButton.scale = [2.0, 2.0, 2.0];
      scene.addNode(playButton);

      // -----------------------------------------------------------------
      // WebXR setup
      // -----------------------------------------------------------------

      function initXR() {
        xrButton = new WebXRButton({
          onRequestSession: onRequestSession,
          onEndSession: onEndSession
        });
        document.querySelector('header').appendChild(xrButton.domElement);

        if (navigator.xr) {
          navigator.xr.isSessionSupported('immersive-vr').then((supported) => {
            xrButton.enabled = supported;
          });

          navigator.xr.requestSession('inline').then(onSessionStarted);
        }
      }

      function initGL() {
        if (gl) return;

        gl = createWebGLContext({ xrCompatible: true });
        document.body.appendChild(gl.canvas);

        function onResize() {
          gl.canvas.width = gl.canvas.clientWidth * window.devicePixelRatio;
          gl.canvas.height = gl.canvas.clientHeight * window.devicePixelRatio;
        }
        window.addEventListener('resize', onResize);
        onResize();

        renderer = new Renderer(gl);
        scene.setRenderer(renderer);
      }

      function onRequestSession() {
        let autoplay = autoplayCheckbox.checked;

        let pending;

        if (autoplay) {
          pending = video.play().then(() => video.pause()).catch(()=>{});
        }

        return navigator.xr.requestSession('immersive-vr', {
          requiredFeatures: ['local-floor']
        }).then((session) => {
          xrButton.setSession(session);
          session.isImmersive = true;
          onSessionStarted(session);

          if (autoplay && pending) {
            pending.then(() => video.play().catch(()=>{}));
          }
        });
      }

      function onSessionStarted(session) {
        session.addEventListener('end', onSessionEnded);
        session.addEventListener('select', (ev) => {
          let refSpace = ev.frame.session.isImmersive ?
            xrImmersiveRefSpace :
            inlineViewerHelper.referenceSpace;
          scene.handleSelect(ev.inputSource, ev.frame, refSpace);
        });

        initGL();
        scene.inputRenderer.useProfileControllerMeshes(session);

        let glLayer = new XRWebGLLayer(session, gl);
        session.updateRenderState({ baseLayer: glLayer });

        let refSpaceType = session.isImmersive ? 'local' : 'viewer';
        session.requestReferenceSpace(refSpaceType).then((refSpace) => {
          if (session.isImmersive) {
            xrImmersiveRefSpace = refSpace;
          } else {
            inlineViewerHelper = new InlineViewerHelper(gl.canvas, refSpace);
          }

          session.requestAnimationFrame(onXRFrame);
        });
      }

      function onEndSession(session) {
        session.end();
      }

      function onSessionEnded(event) {
        if (event.session.isImmersive) {
          xrButton.setSession(null);
          video.pause();
        }
      }

      function onXRFrame(t, frame) {
        let session = frame.session;
        let refSpace = session.isImmersive ?
          xrImmersiveRefSpace :
          inlineViewerHelper.referenceSpace;
        let pose = frame.getViewerPose(refSpace);

        scene.startFrame();
        session.requestAnimationFrame(onXRFrame);
        scene.updateInputSources(frame, refSpace);
        scene.drawXRFrame(frame, pose);
        scene.endFrame();
      }

      initXR();
    </script>
  </body>
</html>